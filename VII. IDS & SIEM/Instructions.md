# 1.1 Exploring Suricata Alerts and Logs: A Hands-on Experience

> Please visit this [link](https://www.coursera.org/learn/detection-and-response) for further information.

> Suricata is a powerful open-source tool that serves as an intrusion detection system (IDS), intrusion prevention system (IPS), and network analysis platform.

> An IDS is a security application designed to monitor system and network activity, identifying and alerting on potential intrusions. These technologies enable organizations to detect signs of malicious activity, helping them safeguard their systems and networks effectively.

## Overview
There are three ways Suricata can be used: 
1. **Intrusion detection system (IDS):** Monitor network traffic and alert on suspicious activities and intrusions. In a practical way, it can be a host-based IDS to monitor system and network activities of a single host like a computer.
2. **Intrusion prevention system (IPS):** Detect and block malicious activity and traffic. It requires additional configuration such as enabling IPS mode.
3. **Network security monitoring (NSM):** Produce and save relevant network logs (live network traffic, existing packet capture files, full or conditional packet captures). This is beneficial for forensics, incident response and for testing signatures.

## Rules 
Rules or signatures are used to identify specific patterns, behavior, and conditions of network traffic that might indicate malicious activity. The terms rule and signature are often used interchangeably in Suricata. Security analysts use **signatures**, or patterns associated with malicious activity, to detect and alert on specific malicious activity. Rules can also be used to provide additional context and visibility into systems and networks, helping to identify potential security threats or vulnerabilities. 

Suricata uses **signatures analysis**, which is a detection method used to find events of interest. Signatures consist of three components:
1. **Action:** The first component of a signature. It describes the action to take if network or system activity matches the signature. Examples include: alert, pass, drop, or reject.
2. **Header:** The header includes network traffic information like source and destination IP addresses, source and destination ports, protocol, and traffic direction.
3. **Rule options:** The rule options provide you with different options to customize signatures.

![Suricata signature](https://github.com/user-attachments/assets/4f5f6188-b2fb-4a6e-a90c-edca0aeee97a)

### Scenario

In this scenario, you’re a security analyst who must monitor traffic on your employer's network. You’ll be required to configure Suricata and use it to trigger alerts.

Here’s how you'll do this task: **First**, you'll explore custom rules in Suricata. **Second**, you'll run Suricata with a custom rule in order to trigger it, and examine the output logs in the `fast.log` file. **Finally**, you’ll examine the additional output that Suricata generates in the standard `eve.json` log file.

For the purposes of the tests you’ll run in this lab activity, you’ve been supplied with a `sample.pcap` file and a `custom.rules` file. These reside in your home folder.

Let’s define the files: 

* The `sample.pcap` file is a packet capture file that contains an example of network traffic data, which you’ll use to test the Suricata rules. This will allow you to simulate and repeat the exercise of monitoring network traffic.
  
* The `custom.rules` file contains a custom rule when the lab activity starts. You’ll add rules to this file and run them against the network traffic data in the `sample.pcap` file.

* The `fast.log` file will contain the alerts that Suricata generates. The `fast.log` file is empty when the lab starts. Each time you test a rule, or set of rules, against the sample network traffic data, Suricata adds a new alert line to the `fast.log` file when all the conditions in any of the rules are met. The `fast.log` file can be located in the `/var/log/suricata` directory after Suricata runs.The `fast.log` file is considered to be a depreciated format and is not recommended for incident response or threat hunting tasks but can be used to perform quick checks or tasks related to quality assurance.

* The `eve.json` file is the main, standard, and default log for events generated by Suricata. It contains detailed information about alerts triggered, as well as other network telemetry events, in JSON format. The `eve.json` file is generated when Suricate runs, and can also be located in the `/var/log/suricata` directory.

When you create a new rule, you'll need to test the rule to confirm whether or not it worked as expected. You can use the `fast.log` file to quickly compare the number of alerts generated each time you run Suricata to test a signature against the `sample.pcap` file.

#### Task 1. Examine a Custom Rule in Suricata

The `/home/analyst` directory contains a `custom.rules` file that defines the network traffic rules, which Suricata captures.

In this task, you’ll explore the composition of the Suricata rule defined in the custom.rules file.

#### Task 2. Trigger a Custom Rule in Suricata

Now that you are familiar with the composition of the custom Suricata rule, you must trigger this rule and examine the alert logs that Suricata generates.

#### Task 3. Examine eve.json Output

In this task, you must examine the additional output that Suricata generates in the eve.json file.

As previously mentioned, this file is located in the /var/log/suricata/ directory.

The eve.json file is the standard and main Suricata log file and contains a lot more data than the fast.log file. This data is stored in a JSON format, which makes it much more useful for analysis and processing by other applications.

#### Expectations 

**By completing this activity, you will:**

1. Gain practical experience in running Suricata to:
  * Create custom rules and execute them in Suricata.
  * Monitor traffic captured in a packet capture file.
  * Examine the fast.log and eve.json output.
2. Develop a key skill essential for your journey toward becoming a security analyst.

# SIEM - Splunk 

> Please visit this [link](https://www.coursera.org/learn/detection-and-response) for further information.

> Splunk is a platform that empowers organizations to prevent major issues, detect threats, restore services, and drive transformation by providing the visibility and insights they require.

> SIEM, such as an Splunk, is an important part of a security analyst's toolbox because it provides a platform for storing, analyzing, and reporting on data from different sources. The Splunk's querying language, called **Search Processing Language (SPL)**, includes the use of **pipes** and **wildcards**. In addition, the effective search helps us efficiently identify patterns, trends, and anomalies within data.

## Scenario 
You are a security analyst working at the e-commerce store Buttercup Games. You've been tasked with identifying whether there are any possible security issues with the mail server. To do so, you must explore any failed SSH logins for the root account.  

## Step-By-Step Instructions

Follow the instructions and answer the following questions to complete the activity.

### Step 1: Access Supporting Materials

The following supporting materials will help you complete this activity. The data contains log and event information from Buttercup Games' mail servers and web accounts. This includes information like access and authentication logs, email logs, and more.

Link to supporting materials: [Tutorialdata.zip](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Tutorialdata.zip) file

### Step 2: Create a Splunk Cloud Account

To use Splunk Cloud, you must create an account. Follow *Part 1 - Create a Splunk Cloud account and Part 2 - Verify your email* in the 
Follow-along guide for [Splunk sign-up](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Splunk%20Sign-Up.pdf) to create an account.
 
### Step 3: Signup for a Free Splunk Cloud Trial

After you've created your Splunk account, you'll need to sign up for a free Splunk Cloud trial. Follow Part 3 - Activate a Splunk Cloud trial in the 
Follow-along guide for [Splunk sign-up](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Splunk%20Sign-Up.pdf).

### Step 4: Upload Data into Splunk

To operate effectively, it's essential that SIEM tools ingest and index data. SIEM tools collect and process data so that it becomes searchable events that can be queried, viewed, and analyzed.

So far, you've created a Splunk account and activated and accessed the Splunk Cloud free trial, but your Splunk Cloud instance does not contain any data. Next, you'll need to upload data into Splunk to start querying. Complete the following steps to upload data into Splunk:

1. If you haven't already, download the data file from **Step 1:** [Tutorialdata.zip](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Tutorialdata.zip). Click the link then click the download icon. Do not uncompress the file.
2. Navigate to Splunk Home from your Splunk Cloud free trial instance. You might need to log in again using your credentials from Step 3.
3. On the Splunk bar, click **Settings**. Then click the **Add Data** icon.
4. Click **Upload**.
5. Click the **Select File** button.
6. Upload the **`tutorialdata.zip`** file, and click **Open**.
7. Click the **Next** button to continue to **Input Settings**.
8. By the **Host** section, select **Segment in path** and enter **1** as the segment number.
9. Click the **Review** button and review the details of the upload before you submit. The details should be as follows: 

    * Input Type: Uploaded File
  
    * File Name: Tutorialdata.zip
  
    * Source Type: Automatic
  
    * Host: Source path segment number: 1 
  
    * Index: Default
  
11. Click **Submit**. Once Splunk has ingested the data, you will receive confirmation that the file was successfully uploaded.

### Step 5: Perform a Basic Search

Take a moment to examine the Splunk Cloud interface by locating the app panel, the Explore Splunk panel, and the Splunk bar.

![Basic Search](https://github.com/user-attachments/assets/06de1e85-82df-49bd-9aed-2ecca6b98733)

Now that you've uploaded the data into Splunk, perform your first query to confirm that the data has been ingested, indexed, and is searchable. Follow these steps to perform a query:

1. Navigate to Splunk Home. (To return to Splunk Home, click the Splunk Cloud logo on the Splunk Cloud page.)
2. Click **Search & Reporting**. You may close any pop ups that appear.
3. In the search bar,  enter your search query:
    **`index="main"`**
    This search term specifies the index. An **index** is a repository for data. Here, the index is a single dataset containing events from an index named main.
4. Select **All Time** from the time range dropdown to search for all the events across all time.
5. Click the search button. Note that the search button is represented by the magnifying glass icon. Your search should retrieve thousands of events.

![Basic Search](https://github.com/user-attachments/assets/cc69b84b-00da-4543-9cdf-d915d14b5c51)

### Step 6: Evaluate the Fields

When Splunk indexes data, it attaches fields to each event. These fields become part of the searchable index event data. This helps security analysts easily search for and find the specific data they need. Now that you've run your first query, examine the search results and the fields.

For each event the fields are **`host`, `source`,** and **`sourcetype`**. Under **SELECTED FIELDS**, examine the same fields.

![Select Fields](https://github.com/user-attachments/assets/4e45e776-3711-48a7-ac66-3e268a294265)

Examine the field values by clicking on the field under **SELECTED FIELDS**. You should observe the following:

* **host:** The host field specifies the name of the network host from which the event originated. In this search there are five hosts:

  * **`mailsv`** - Buttercup Games' mail server. Examine events generated from this host.

  * **`www1`** - This is one of Buttercup Games' web applications.

  * **`www2`** - This is one of Buttercup Games' web applications.

  * **`www3`** - This is one of Buttercup Games' web applications.

  * **`vendor_sales`** - Information about Buttercup Games' retail sales.

* **source:** The source field indicates the file name from which the event originates. You should identify eight sources. Notice **`/mailsv/secure.log`**, which is a log file that contains information related to authentication and authorization attempts on the mail server.

* **sourcetype:** The sourcetype determines how data is formatted. You should observe three sourcetypes. Examine **`secure-2`**.

### Step 7: Narrow Your Search

Because you've been tasked with exploring any failed SSH logins for the root account on the mail server, you'll need to narrow the search results for events from the mail server.

Under **SELECTED FIELDS**, click **host** and click **mailsv**.

Notice that a new term has been added to the search bar: **`index=main host=mailsv`**. The search results have narrowed to over 9000 events that are generated by the mail server.

### Step 8: Search for a Failed Login for Root
Now that you've narrowed your search results to events generated by the mail server, continue to narrow the search to locate any failed SSH logins for the **root** account. 

1. Clear the search bar.

2. Enter **`index=main host=mailsv fail* root`** into the search bar. This search expands on the search from the previous task and searches for the keyword **`fail*`**. The wildcard tells Splunk to expand the search term to find other terms that contain the word *fail* such as *failure*, *failed*, etc. Lastly, the keyword **`root`** searches for any event that contains the term root.

Click **search**.

### Step 9: Evaluate the Search Results

Your search from the previous task should have retrieved search results for over 300 events. Navigate to other pages of the search results to observe the events not listed on the first page of results.

### Expectation

**By completing this activity, you will:**

  * Upload and process sample log data.
  * Perform effective searches on indexed data.
  * Analyze and evaluate search results.
  * Identify and differentiate between various data sources.
  * Detect failed SSH login attempts for the root account.

# SIEM - Chronicle

> Please visit this [link](https://www.coursera.org/learn/detection-and-response) for further information.

>  SIEM is a security application that collects, analyzes, and monitors log data to track critical activities and detect potential threats within an organization.

>  Chronicle is a cloud-native service built on Google’s infrastructure, designed for enterprises. It allows organizations to securely store, analyze, and search through large volumes of security and network telemetry data to enhance threat detection and response capabilities. 

## Overview 

In Chronicle, the **Search** field allows you to look for specific events across your data. **Procedural Filtering** can be applied to narrow down search results by adding filters that include or exclude particular event types or log sources. For example, it can be used to focus on specific event characteristics or sources of log data.

Additionally, **YARA-L** is a specialized language used to create rules that help search through the ingested log data. These rules can be tailored to identify patterns or specific events of interest, enhancing the efficiency and accuracy of searches.

There are two types of searches in Chronicle: **Unified Data Mode (UDM)** and **Raw Log Search**.

**Unified Data Mode (UDM)** is the default search type in Chronicle. It searches through security data that has been ingested, parsed, and normalized. UDM searches are faster because the data is indexed and structured, making it more efficient for retrieving results.

**Raw Log Search** searches through unparsed, raw logs. This search type is slower than UDM because it works with raw data. However, it offers greater flexibility, allowing users to specify details like usernames, filenames, hashes, and more. It also supports regular expressions to refine searches and match specific patterns in the log data.

## Scenario 

You are a security analyst at a financial services company. You receive an alert that an employee received a phishing email in their inbox. You review the alert and identify a suspicious domain name contained in the email's body: `signin.office365x24.com`. You need to determine whether any other employees have received phishing emails containing this domain and whether they have visited the domain. 

## Step-By-Step Instructions
Follow the instructions and answer the series of questions to complete the activity.

### Step 1 Launch Chronicle

Click the link to launch [Chronicle](https://demo.backstory.chronicle.security/?warstory=).

On the Chronicle home page, you’ll find the current date and time, a search bar, and details about the total number of log entries. There are already a significant number of log events ingested into the Chronicle instance.

![image](https://github.com/user-attachments/assets/8b9b121e-b8cc-46d8-b2be-5769235e79dc)

### Step 2: Perfom a Domain Search

To begin, complete these steps to perform a domain search for the domain contained in the phishing email. Then, search for events using information like hostnames, domains, IP addresses, URLs, email addresses, usernames, and file hashes. 

1. In the search bar, type **`signin.office365x24.com`** and click **Search**. Under **DOMAINS**, **`signin.office365x24.com`** will be listed. This tells you that the domain exists in the ingested data. 

2. Click **`signin.office365x24.com`** to complete the search.

3. Click **`Go to Legacy View`** to use the original chronicle interface.

### Step 3: Evaluate the Search Results

After performing a domain search, you'll be in the domain view. Evaluate the search results and observe the following:

1. **VT CONTEXT:** This section provides the VirusTotal information available for the domain. 

2. **WHOIS:** This section provides a summary of information about the domain using WHOIS, a free and publicly available directory that includes information about registered domain names, such as the name and contact information of the domain owner. In cybersecurity, this information is helpful in assessing a domain's reputation and determining the origin of malicious websites. 

3. **Prevalence:** This section provides a graph which outlines the historical prevalence of the domain. This can be helpful when you need to determine whether the domain has been accessed previously. Usually, less prevalent domains may indicate a greater threat. 

4. **RESOLVED IPS:** This insight card provides additional context about the domain, such as the IP address that maps to **`signin.office365x24.com`**, which is **`40.100.174.34`**. Clicking on this IP will run a new search for the IP address in Chronicle. Insight cards can be helpful in expanding the domain investigation and further investigating an indicator to determine whether there is a broader compromise.

5. **SIBLING DOMAINS:** This insight card provides additional context about the domain. Sibling domains share a common top or parent domain. For example, here the sibling domain is listed as **`login.office365x24.com`**, which shares the same top domain **`office365x24.com`** with the domain you're investigating: **`signin.office365x24.com`**.

6. Click **TIMELINE**. This tab provides information about the events and interactions made with this domain. Click **EXPAND ALL** to reveal the details about the HTTP requests made including GET and POST requests.  A **`GET`** request retrieves data from a server while a **`POST`** request submits data to a server.

7. Click **ASSETS**. This tab provides a list of the assets that have accessed the domain.

![Evaluate the Search Results](https://github.com/user-attachments/assets/a8379bd6-63b1-4e0f-a484-58320f63e32e)

### Step 4: Investigate the Threat Intelligence Data

Now that you've retrieved results for the domain name, the next step is to determine whether the domain is malicious. Chronicle provides quick access to threat intelligence data from the search results that you can use to help your investigation. Follow these steps to analyze the threat intelligence data and use your incident handler's journal to record interesting data:

1. Click on **VT CONTEXT** to analyze the available VirusTotal information about this domain. There is no VirusTotal information about this domain. To exit the VT CONTEXT window, click the **X**.

2. By **Top Private Domain**, click **`office365x24.com`** to access the domain view for **`office365x24.com`**. Click **VT CONTEXT** to assess the VirusTotal information about this domain. In the pop up, you can observe that one vendor has flagged this domain as malicious. Exit the VT CONTEXT window. Click the back button in your browser to go back to the domain view for the **`signin.office365x24.com`** search.

### Step 5: Investigate the Affected Assests and Events

Information about the events and assets relating to the domain are separated into the two tabs: TIMELINE and ASSETS. TIMELINE shows the timeline of events that includes when each asset accessed the domain. ASSETS list hostnames, IP addresses, MAC addresses, or devices that have accessed the domain. 

Investigate the affected assets and events by exploring the tabs:

1. **ASSETS:** There are several different assets that have accessed the domain, along with the date and time of access. Using your incident handler's journal, record the name and number of assets that have accessed the domain. 

2. **TIMELINE:** Click **EXPAND ALL** to reveal the details about the HTTP requests made, including **`GET`** and **`POST`** requests. The **`POST`** information is especially useful because it means that data was sent to the domain. It also suggests a possible successful phish. Using your incident handler's journal, take note of the **`POST`** requests to the **`/login.php`** page. For more details about the connections, open the raw log viewer by clicking the open icon.

![Investigate the Affected Assests and Events](https://github.com/user-attachments/assets/e48dbc08-5b49-4baf-af2c-b4f1ae09e5f8)

###  Step 6: Investigate the Resolved IP Address

So far, you have collected information about the domain's reputation using threat intelligence, and you've identified the assets and events associated with the domain. Based on this information, it's clear that this domain is suspicious and most likely malicious. But before you can confirm that it is malicious, there's one last thing to investigate.

Attackers sometimes reuse infrastructure for multiple attacks. In these cases, multiple domain names resolve to the same IP address.

Investigate the IP address found under the **RESOLVED IPS** insight card to identify if the **`signin.office365x24.com`** domain uses another domain. Follow these steps: 

1. Under **`RESOLVED IPS`**, click the IP address 40.100.174.34.

2. Evaluate the search results for this IP address and use your incident handler's journal to take note of the following:

    a. **TIMELINE:** Take note of the additional POST request. A new POST suggests that an asset may have been phished.

    b. **ASSETS:** Take note of the additional affected assets.

    c. **DOMAINS:** Take note of the additional domains associated with this IP address.

### Expectation 

**By completing this activity, you will:**

1. Review threat intelligence reports related to the domain.
2. Identify assets that have accessed the domain.
3. Analyze HTTP events associated with the domain.
4. Determine which assets submitted login credentials to the domain.
5. Identify other related domains.







