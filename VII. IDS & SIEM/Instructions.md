# 1.1 Exploring Suricata Alerts and Logs: A Hands-on Experience

> Please visit this [link](https://www.coursera.org/learn/detection-and-response) for further information.

> Suricata is a powerful open-source tool that serves as an intrusion detection system (IDS), intrusion prevention system (IPS), and network analysis platform.

> An IDS is a security application designed to monitor system and network activity, identifying and alerting on potential intrusions. These technologies enable organizations to detect signs of malicious activity, helping them safeguard their systems and networks effectively.

## Overview
There are three ways Suricata can be used: 
1. **Intrusion detection system (IDS):** Monitor network traffic and alert on suspicious activities and intrusions. In a practical way, it can be a host-based IDS to monitor system and network activities of a single host like a computer.
2. **Intrusion prevention system (IPS):** Detect and block malicious activity and traffic. It requires additional configuration such as enabling IPS mode.
3. **Network security monitoring (NSM):** Produce and save relevant network logs (live network traffic, existing packet capture files, full or conditional packet captures). This is beneficial for forensics, incident response and for testing signatures.

## Rules 
Rules or signatures are used to identify specific patterns, behavior, and conditions of network traffic that might indicate malicious activity. The terms rule and signature are often used interchangeably in Suricata. Security analysts use **signatures**, or patterns associated with malicious activity, to detect and alert on specific malicious activity. Rules can also be used to provide additional context and visibility into systems and networks, helping to identify potential security threats or vulnerabilities. 

Suricata uses **signatures analysis**, which is a detection method used to find events of interest. Signatures consist of three components:
1. **Action:** The first component of a signature. It describes the action to take if network or system activity matches the signature. Examples include: alert, pass, drop, or reject.
2. **Header:** The header includes network traffic information like source and destination IP addresses, source and destination ports, protocol, and traffic direction.
3. **Rule options:** The rule options provide you with different options to customize signatures.

![Suricata signature](https://github.com/user-attachments/assets/4f5f6188-b2fb-4a6e-a90c-edca0aeee97a)

### Scenario

In this scenario, you’re a security analyst who must monitor traffic on your employer's network. You’ll be required to configure Suricata and use it to trigger alerts.

Here’s how you'll do this task: **First**, you'll explore custom rules in Suricata. **Second**, you'll run Suricata with a custom rule in order to trigger it, and examine the output logs in the `fast.log` file. **Finally**, you’ll examine the additional output that Suricata generates in the standard `eve.json` log file.

For the purposes of the tests you’ll run in this lab activity, you’ve been supplied with a `sample.pcap` file and a `custom.rules` file. These reside in your home folder.

Let’s define the files: 

* The `sample.pcap` file is a packet capture file that contains an example of network traffic data, which you’ll use to test the Suricata rules. This will allow you to simulate and repeat the exercise of monitoring network traffic.
  
* The `custom.rules` file contains a custom rule when the lab activity starts. You’ll add rules to this file and run them against the network traffic data in the `sample.pcap` file.

* The `fast.log` file will contain the alerts that Suricata generates. The `fast.log` file is empty when the lab starts. Each time you test a rule, or set of rules, against the sample network traffic data, Suricata adds a new alert line to the `fast.log` file when all the conditions in any of the rules are met. The `fast.log` file can be located in the `/var/log/suricata` directory after Suricata runs.The `fast.log` file is considered to be a depreciated format and is not recommended for incident response or threat hunting tasks but can be used to perform quick checks or tasks related to quality assurance.

* The `eve.json` file is the main, standard, and default log for events generated by Suricata. It contains detailed information about alerts triggered, as well as other network telemetry events, in JSON format. The `eve.json` file is generated when Suricate runs, and can also be located in the `/var/log/suricata` directory.

When you create a new rule, you'll need to test the rule to confirm whether or not it worked as expected. You can use the `fast.log` file to quickly compare the number of alerts generated each time you run Suricata to test a signature against the `sample.pcap` file.

#### Task 1. Examine a Custom Rule in Suricata

The `/home/analyst` directory contains a `custom.rules` file that defines the network traffic rules, which Suricata captures.

In this task, you’ll explore the composition of the Suricata rule defined in the custom.rules file.

#### Task 2. Trigger a Custom Rule in Suricata

Now that you are familiar with the composition of the custom Suricata rule, you must trigger this rule and examine the alert logs that Suricata generates.

#### Task 3. Examine eve.json Output

In this task, you must examine the additional output that Suricata generates in the eve.json file.

As previously mentioned, this file is located in the /var/log/suricata/ directory.

The eve.json file is the standard and main Suricata log file and contains a lot more data than the fast.log file. This data is stored in a JSON format, which makes it much more useful for analysis and processing by other applications.

#### Expectations 

**By completing this activity, you will:**

1. Gain practical experience in running Suricata to:
  * Create custom rules and execute them in Suricata.
  * Monitor traffic captured in a packet capture file.
  * Examine the fast.log and eve.json output.
2. Develop a key skill essential for your journey toward becoming a security analyst.

# SIEM - Splunk 

> Please visit this [link](https://www.coursera.org/learn/detection-and-response) for further information.

> Splunk is a platform that empowers organizations to prevent major issues, detect threats, restore services, and drive transformation by providing the visibility and insights they require.

## Overview 
SIEM, such as an Splunk, is an important part of a security analyst's toolbox because it provides a platform for storing, analyzing, and reporting on data from different sources. The Splunk's querying language, called **Search Processing Language (SPL)**, includes the use of **pipes** and **wildcards**. In addition, the effective search helps us efficiently identify patterns, trends, and anomalies within data. 

## Scenario 
You are a security analyst working at the e-commerce store Buttercup Games. You've been tasked with identifying whether there are any possible security issues with the mail server. To do so, you must explore any failed SSH logins for the root account.  

## Step-By-Step Instructions

Follow the instructions and answer the following questions to complete the activity.

### Step 1: Access Supporting Materials

The following supporting materials will help you complete this activity. The data contains log and event information from Buttercup Games' mail servers and web accounts. This includes information like access and authentication logs, email logs, and more.

Link to supporting materials: [Tutorialdata.zip](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Tutorialdata.zip) file

## Step 2: Create a Splunk Cloud Account

To use Splunk Cloud, you must create an account. Follow *Part 1 - Create a Splunk Cloud account and Part 2 - Verify your email* in the 
Follow-along guide for [Splunk sign-up](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Splunk%20Sign-Up.pdf) to create an account.
 
## Step 3: Signup for a Free Splunk Cloud Trial

After you've created your Splunk account, you'll need to sign up for a free Splunk Cloud trial. Follow Part 3 - Activate a Splunk Cloud trial in the 
Follow-along guide for [Splunk sign-up](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Splunk%20Sign-Up.pdf).

## Step 4: Upload Data into Splunk

To operate effectively, it's essential that SIEM tools ingest and index data. SIEM tools collect and process data so that it becomes searchable events that can be queried, viewed, and analyzed.

So far, you've created a Splunk account and activated and accessed the Splunk Cloud free trial, but your Splunk Cloud instance does not contain any data. Next, you'll need to upload data into Splunk to start querying. Complete the following steps to upload data into Splunk:

1. If you haven't already, download the data file from **Step 1:** [Tutorialdata.zip](https://github.com/Hugh-Kumbi/Cybersecurity-Portfolio/blob/main/VII.%20IDS%20%26%20SIEM/Tutorialdata.zip). Click the link then click the download icon. Do not uncompress the file.
2. Navigate to Splunk Home from your Splunk Cloud free trial instance. You might need to log in again using your credentials from Step 3.
3. On the Splunk bar, click **Settings**. Then click the **Add Data** icon.
4. Click **Upload**.
5. Click the **Select File** button.
6. Upload the **`tutorialdata.zip`** file, and click **Open**.
7. Click the **Next** button to continue to **Input Settings**.
8. By the **Host** section, select **Segment in path** and enter **1** as the segment number.
9. Click the **Review** button and review the details of the upload before you submit. The details should be as follows: 

    * Input Type: Uploaded File
  
    * File Name: Tutorialdata.zip
  
    * Source Type: Automatic
  
    * Host: Source path segment number: 1 
  
    * Index: Default
  
11. Click **Submit**. Once Splunk has ingested the data, you will receive confirmation that the file was successfully uploaded.

## Step 5: Perform a Basic Search

Take a moment to examine the Splunk Cloud interface by locating the app panel, the Explore Splunk panel, and the Splunk bar.

![Basic Search](https://github.com/user-attachments/assets/06de1e85-82df-49bd-9aed-2ecca6b98733)

Now that you've uploaded the data into Splunk, perform your first query to confirm that the data has been ingested, indexed, and is searchable. Follow these steps to perform a query:

1. Navigate to Splunk Home. (To return to Splunk Home, click the Splunk Cloud logo on the Splunk Cloud page.)
2. Click **Search & Reporting**. You may close any pop ups that appear.
3. In the search bar,  enter your search query:
    **`index="main"`**
    This search term specifies the index. An **index** is a repository for data. Here, the index is a single dataset containing events from an index named main.
4. Select **All Time** from the time range dropdown to search for all the events across all time.
5. Click the search button. Note that the search button is represented by the magnifying glass icon. Your search should retrieve thousands of events.

![Basic Search](https://github.com/user-attachments/assets/cc69b84b-00da-4543-9cdf-d915d14b5c51)

## Step 6: Evaluate the Fields

When Splunk indexes data, it attaches fields to each event. These fields become part of the searchable index event data. This helps security analysts easily search for and find the specific data they need. Now that you've run your first query, examine the search results and the fields.

For each event the fields are **`host`, `source`,** and **`sourcetype`**. Under **SELECTED FIELDS**, examine the same fields.

![Select Fields](https://github.com/user-attachments/assets/4e45e776-3711-48a7-ac66-3e268a294265)

Examine the field values by clicking on the field under **SELECTED FIELDS**. You should observe the following:

* **host:** The host field specifies the name of the network host from which the event originated. In this search there are five hosts:

  * **`mailsv`** - Buttercup Games' mail server. Examine events generated from this host.

  * **`www1`** - This is one of Buttercup Games' web applications.

  * **`www2`** - This is one of Buttercup Games' web applications.

  * **`www3`** - This is one of Buttercup Games' web applications.

  * **`vendor_sales`** - Information about Buttercup Games' retail sales.

* **source:** The source field indicates the file name from which the event originates. You should identify eight sources. Notice **`/mailsv/secure.log`**, which is a log file that contains information related to authentication and authorization attempts on the mail server.

* **sourcetype:** The sourcetype determines how data is formatted. You should observe three sourcetypes. Examine **`secure-2`**.

## Step 7: Narrow Your Search

Because you've been tasked with exploring any failed SSH logins for the root account on the mail server, you'll need to narrow the search results for events from the mail server.

Under **SELECTED FIELDS**, click **host** and click **mailsv**.

Notice that a new term has been added to the search bar: **`index=main host=mailsv`**. The search results have narrowed to over 9000 events that are generated by the mail server.

## Step 8: Search for a Failed Login for Root
Now that you've narrowed your search results to events generated by the mail server, continue to narrow the search to locate any failed SSH logins for the **root** account. 

1. Clear the search bar.

2. Enter **`index=main host=mailsv fail* root`** into the search bar. This search expands on the search from the previous task and searches for the keyword **`fail*`**. The wildcard tells Splunk to expand the search term to find other terms that contain the word *fail* such as *failure*, *failed*, etc. Lastly, the keyword **`root`** searches for any event that contains the term root.

Click **search**.

## Step 9: Evaluate the Search Results

Your search from the previous task should have retrieved search results for over 300 events. Navigate to other pages of the search results to observe the events not listed on the first page of results.

### Expectation

**By completing this activity, you will:**

  * Upload and process sample log data.
  * Perform effective searches on indexed data.
  * Analyze and evaluate search results.
  * Identify and differentiate between various data sources.
  * Detect failed SSH login attempts for the root account.
